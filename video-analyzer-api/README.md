# Video Analyzer API

API для анализа видео и автоматического выделения сцен с использованием AI-моделей.

## Возможности

- Автоматическое обнаружение сцен в видео
- Анализ аудио каждой сцены и извлечение транскрипций
- Группировка сцен в сюжетные линии
- Предоставление таймкодов для каждой сцены и сюжетной линии
- Асинхронная обработка видео в фоновом режиме

## Требования

- Docker и Docker Compose
- NVIDIA GPU с установленным NVIDIA Container Toolkit (для работы с GPU)
- Около 15 ГБ свободного места для кэша моделей (зависит от выбранных моделей)

## Настройка и запуск

### 1. Создание файла .env

Создайте файл `.env` на основе примера `.env.example`:

```bash
cp .env.example .env
```

Отредактируйте `.env` в соответствии с вашими потребностями:

- Для работы на CPU установите для всех моделей `*_DEVICE=cpu` и соответствующие `*_COMPUTE_TYPE`
- Для экономии ресурсов используйте меньшие модели (например, `WHISPER_MODEL_SIZE=tiny`)
- Для лучшего качества используйте большие модели (например, `WHISPER_MODEL_SIZE=large-v2`)

### 2. Запуск с Docker Compose

```bash
docker-compose up
```

При первом запуске будут автоматически загружены все необходимые модели. Это может занять некоторое время в зависимости от скорости интернета и выбранных моделей.

Все скачанные модели сохраняются в директории `~/.cache/huggingface` и используются повторно при последующих запусках.

## Проверка API

После запуска API будет доступно по адресу: http://localhost:8000

Проверить работоспособность можно с помощью:

```bash
curl http://localhost:8000
```

## Работа с моделями

Система использует следующие AI-модели:

1. **BLIP2** (Salesforce/blip2-opt-2.7b) - для генерации описаний сцен
2. **CLIP** (openai/clip-vit-base-patch32) - для анализа кадров и сопоставления сцен с сюжетами
3. **Whisper** (разные размеры) - для транскрипции аудио
4. **RuBERT** (DeepPavlov/rubert-base-cased) - для семантического анализа текста

Все модели настраиваются через переменные окружения в файле `.env` и автоматически сохраняются в локальном кэше `~/.cache/huggingface`, который монтируется в контейнер. Это позволяет:

- Избежать повторной загрузки моделей при перезапуске контейнера
- Использовать один и тот же кэш между разными проектами
- Предварительно загружать модели вручную
- Гибко настраивать используемые модели и режимы их работы

### Предварительная загрузка моделей

Если вы хотите загрузить модели заранее, вы можете использовать Python:

```bash
# Загрузка BLIP2
python3 -c "from transformers import Blip2Processor, Blip2ForConditionalGeneration; import torch; processor = Blip2Processor.from_pretrained('Salesforce/blip2-opt-2.7b'); model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')"

# Загрузка Whisper
python3 -c "from faster_whisper import WhisperModel; model = WhisperModel('small')"
```

## Оптимизация производительности

- Для лучшей производительности используйте GPU
- Для экономии ресурсов используйте маленькие модели (`tiny` или `small`)
- Для лучшего качества анализа используйте большие модели, но учитывайте, что они требуют больше памяти

## Решение проблем

### Нет свободного места

Если у вас заканчивается место на диске из-за кэша моделей, вы можете очистить его:

```bash
rm -rf ~/.cache/huggingface/models--openai--whisper-large-v2
```

### Ошибка CUDA

Если вы видите ошибки, связанные с CUDA, но у вас нет GPU, измените настройки в файле `.env`:

```
BLIP2_DEVICE=cpu
VISION_DEVICE=cpu
WHISPER_DEVICE=cpu
BLIP2_COMPUTE_TYPE=float32
VISION_COMPUTE_TYPE=float32
WHISPER_COMPUTE_TYPE=int8
```

## API-эндпоинты

### Получение списка доступных видео

```
GET /api/sample-videos
```

**Ответ:**

```json
{
  "videos": ["video1.mp4", "video2.mp4"]
}
```

### Запуск анализа видео

```
POST /api/analyze
```

**Тело запроса:**

```json
{
  "filename": "video1.mp4",
  "num_storylines": 3
}
```

**Ответ:**

```json
{
  "task_id": "video1_3",
  "status": "processing",
  "message": "Анализ видео запущен в фоновом режиме"
}
```

### Проверка статуса анализа

```
GET /api/analysis/{task_id}
```

**Ответ для незавершенной задачи:**

```json
{
  "status": "processing",
  "message": "Обнаружение сцен...",
  "progress": 0.2,
  "last_updated": "2023-07-15T14:30:45.123456"
}
```

**Ответ для завершенной задачи:**

```json
{
  "status": "completed",
  "task_id": "video1_3",
  "result": {
    "video_filename": "video1.mp4",
    "duration": 360.5,
    "total_scenes": 24,
    "storylines": [
      {
        "id": "storyline_1",
        "name": "Сюжетная линия 1",
        "description": "Сюжет длительностью 45.5 секунд, включающий 3 сцены",
        "scenes": [
          {
            "id": "scene_1",
            "start_time": 120.0,
            "end_time": 165.5,
            "duration": 45.5,
            "start_frame": 2880,
            "end_frame": 3972,
            "audio_analysis": {
              "transcript": "Текстовая транскрипция диалога в сцене",
              "language": "ru",
              "audio_features": {
                "rms_energy": 0.12,
                "spectral_centroid_mean": 1560.4,
                "zero_crossing_rate": 0.05,
                "tempo": 95.6
              },
              "speakers": null,
              "emotions": null
            }
          }
        ],
        "duration": 45.5,
        "start_time": 120.0,
        "end_time": 165.5
      }
    ],
    "segments": [
      {
        "scene_id": "scene_1",
        "start_time": 121.5,
        "end_time": 124.2,
        "text": "Привет, как твои дела?",
        "start_in_scene": 1.5,
        "end_in_scene": 4.2
      },
      {
        "scene_id": "scene_1",
        "start_time": 125.0,
        "end_time": 128.3,
        "text": "Всё хорошо, спасибо. А у тебя?",
        "start_in_scene": 5.0,
        "end_in_scene": 8.3
      }
    ],
    "timestamp": "2023-07-15T14:35:10.123456",
    "metadata": {
      "fps": 24,
      "size": [1920, 1080],
      "analysis_time_seconds": 25.8
    }
  }
}
```

## Принцип работы

1. API принимает запрос на анализ видео из директории `shared-data/sample-videos/`
2. Запускает асинхронный процесс анализа:
   - Извлечение метаданных видео
   - Обнаружение сцен с помощью PySceneDetect
   - Анализ аудио каждой сцены и извлечение транскрипций
   - Группировка сцен в сюжетные линии
3. Результаты сохраняются в формате JSON в директории `shared-data/results/`
4. Клиент может опрашивать статус задачи и получать результаты

## Архитектура пайплайна анализа

Новая архитектура сервиса основана на модульном пайплайне анализа:

1. **Класс AnalysisPipeline**: Главный класс, координирующий процесс анализа

   - Инициализирует и управляет всеми анализаторами
   - Последовательно запускает каждый этап анализа
   - Собирает и объединяет результаты всех этапов

2. **Класс SceneDetector**: Отвечает за обнаружение сцен в видео

   - Использует PySceneDetect для поиска границ сцен
   - Возвращает список сцен с метаданными

3. **Класс AudioAnalyzer**: Анализирует аудио для каждой сцены

   - Извлекает аудиосегмент из видео
   - Транскрибирует речь с помощью Whisper
   - Анализирует аудио-характеристики

Такая модульная архитектура позволяет легко добавлять новые анализаторы (например, для лиц, объектов) без изменения существующего кода.

## Детальное описание алгоритма анализа

Анализ видео состоит из нескольких последовательных этапов:

### 1. Функция `analyze_video`

Основная функция, которая организует весь процесс анализа:

- Получает информацию о видеофайле (длительность, FPS, размеры) с помощью MoviePy
- Вызывает функцию обнаружения сцен
- Группирует сцены в сюжетные линии
- Формирует и сохраняет результаты анализа в JSON-файл
- Обновляет статус задачи на протяжении всего процесса

### 2. Функция `detect_scenes`

Отвечает за определение границ сцен в видео:

- Использует библиотеку PySceneDetect
- Применяет ContentDetector с пороговым значением 27.0 (оптимизировано для большинства видео)
- ContentDetector анализирует изменения в содержимом кадров:
  - Для каждой пары последовательных кадров рассчитывается "значение разницы"
  - Если значение превышает порог, обнаруживается новая сцена
- Возвращает список сцен с информацией о начале, конце и длительности каждой сцены

### Будущие улучшения алгоритма

Текущая реализация использует временные и длительностные характеристики сцен, а также анализ аудио. В будущих версиях планируется добавить:

1. ✅ Аудиоанализ и транскрипция речи

   - Транскрипция диалогов с помощью модели Whisper
   - Анализ аудио характеристик для каждой сцены

2. Распознавание персонажей
   Что сделать: Добавить систему распознавания лиц и отслеживания персонажей в видео
   Ожидаемый результат: Список персонажей с временными метками их появления в каждой сцене

3. Семантический анализ сцен
   Что сделать: Разработать алгоритм для анализа содержания сцен и определения их тематической близости
   Ожидаемый результат: Система, группирующая сцены по смыслу, а не только по времени
4. Определение "интересности" сцен
   Что сделать: Создать модель для оценки значимости и интересности каждой сцены
   Ожидаемый результат: Числовая оценка каждой сцены, помогающая выделять ключевые моменты
5. Алгоритм группировки в сюжетные линии
   Что сделать: Разработать алгоритм, объединяющий данные о персонажах, диалогах и семантике для формирования сюжетных линий
   Ожидаемый результат: Автоматическое выделение сюжетных линий, даже если их сцены разделены во времени
6. Система метрик релевантности
   Что сделать: Создать систему оценки принадлежности каждой сцены к сюжетным линиям
   Ожидаемый результат: Показатели релевантности сцен к каждой сюжетной линии
7. Визуализация сюжетных линий
   Что сделать: Разработать интерактивный таймлайн для отображения переплетения сюжетных линий
   Ожидаемый результат: Наглядное представление структуры сюжета с возможностью интерактивного взаимодействия
8. Экспорт сюжетных линий
   Что сделать: Добавить возможность экспорта отдельных сюжетных линий как видеофрагментов
   Ожидаемый результат: Функционал для получения готовых видеонарезок по каждой сюжетной линии
9. Система обратной связи для улучшения
   Что сделать: Внедрить механизм сбора пользовательских корректировок для улучшения моделей
   Ожидаемый результат: Постоянное повышение точности автоматического анализа
10. Адаптация для разных типов контента
    Что сделать: Расширить поддержку для различных жанров и форматов видео
    Ожидаемый результат: Универсальная система, работающая с любым видеоконтентом

Доп комменты:

1. Для анализа сюжетных линий можно строить граф связей между сценами
   💬 Комментарий: можно создать граф сюжетной связи, где:
   узлы — это сцены,
   рёбра — это:
   общие объекты,
   общие персонажи,
   причинно-следственная связь (если найдена через GPT).

2. Сохраняй все объекты и события для поиска по будущим сценам
   💬 Комментарий: при анализе сцены сохраняй список:

   ```
     {
     objects: ['flash drive', 'door', 'safe'],
     people: ['unknown man'],
     actions: ['enter', 'steal', 'run'],
     sceneId: 's12'
     }
   ```

   — Когда ты дойдёшь до сцены 40 и найдёшь «герой говорит: флешка пропала» — ты сможешь найти по ключевому слову "flash drive" прошлые сцены, где это упоминалось / присутствовало.

3. 3.1 Если ты строишь MVP — начинай с:
   3.2 Whisper + GPT анализ текста.
   3.3 Сбор персонажей и упоминаний.
   3.4 Эмбеддинги и группировка похожих сцен.
   3.5 Добавь "экшен-радар" через простую motion-анализ или pre-trained модель.

## Дополнительные настройки

Можно изменить порт и другие настройки в файле `docker-compose.yml`:

```yaml
services:
  video-analyzer:
    ...
    ports:
      - "8000:8000"  # Изменить первое число для другого внешнего порта
    ...
```

План от ChatGPT
https://chatgpt.com/canvas/shared/6817793a78248191841cc6546c445fa8
